{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "https://gist.github.com/HussainBadreddeen/f7a9d839a1dd31259be5dcdf73d721a7#file-hard_dataset_preprocessing-ipynb",
      "authorship_tag": "ABX9TyOo5JSOupcutSItIPRLjZAh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HussainBadreddeen/AutoML_Thesis/blob/main/hard_dataset_preprocessing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Importing libraries needed and importing the dataset from google drive"
      ],
      "metadata": {
        "id": "54Ts7bN441C4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NGVZQ1rp-Q2m"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "%matplotlib inline\n",
        "sns.set()\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "\n",
        "\n",
        "file_path = \"/content/drive/MyDrive/thesisdata/balanced-reviews.txt\"\n",
        "\n",
        "# Load as a DataFrame (change delimiter if needed)\n",
        "df = pd.read_csv(file_path, sep=\"\\t\", encoding=\"utf-16\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## I follow the CRISP-DM process here.\n",
        "I've already done business understanding since I've mentioned the goal and objectives of this project is to compare AutoML for arabic sentiment analysis with manual tuning of hyperparameters"
      ],
      "metadata": {
        "id": "3CVj2ipW6W6W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Data Understanding**"
      ],
      "metadata": {
        "id": "PfnWQ11w5iC4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#first few rows\n",
        "print(df.head())\n",
        "print(\"----------------------------\")\n",
        "print()\n",
        "#Number of Rows and coloumns\n",
        "print('Number of rows and columns in the data set:', df.shape)\n",
        "print(\"----------------------------\")\n",
        "print()\n",
        "#Data types of the dataset\n",
        "df.info()\n",
        "print(\"----------------------------\")\n",
        "print()\n",
        "\n",
        "# Display summary statistics for numerical features\n",
        "print(df.describe())\n",
        "print(\"----------------------------\")\n",
        "print()\n",
        "#quick check if there are any missing values\n",
        "print(\"Missing Values:\")\n",
        "print(df.isnull().sum())\n",
        "print(\"----------------------------\")\n",
        "print()"
      ],
      "metadata": {
        "id": "8H5Z2T_aHe8b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##since we know that we are trying to measure sentiment we focus on data related to reviews and their ratings"
      ],
      "metadata": {
        "id": "ajGxzFiC8zld"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Checking How many values are Unqiue from the total values for the Column rating\n",
        "print(\"Amount of Values in the column:\", df[\"rating\"].count())\n",
        "print(\"----------------------------\")\n",
        "unique_ratings = df['rating'].nunique()\n",
        "print(f\"Number of unique ratings: {unique_ratings}\")\n",
        "print(\"The unique values are: \" ,df['rating'].unique())\n",
        "#1,2,4,5 are the unique values as they represent either negative (1,2) or positive (4,5). 3 was removed in the balanced version of the dataset\n",
        "print(\"----------------------------\")\n",
        "\n",
        "count_of_rating_1 = (df['rating'] == 1).sum()\n",
        "print(\"Count of 'rating one':\", count_of_rating_1, \"                   Percentage of Total=\", count_of_rating_1/df[\"rating\"].count()*100,\"%\")\n",
        "print(\"----------------------------\")\n",
        "\n",
        "count_of_rating_2 = (df['rating'] == 2).sum()\n",
        "print(\"Count of 'rating two':\", count_of_rating_2, \"                   Percentage of Total=\", count_of_rating_2/df[\"rating\"].count()*100,\"%\")\n",
        "print(\"----------------------------\")\n",
        "\n",
        "count_of_rating_4 = (df['rating'] == 4).sum()\n",
        "print(\"Count of 'rating four':\", count_of_rating_4, \"                   Percentage of Total=\", count_of_rating_4/df[\"rating\"].count()*100,\"%\")\n",
        "print(\"----------------------------\")\n",
        "\n",
        "count_of_rating_5 = (df['rating'] == 5).sum()\n",
        "print(\"Count of 'rating five':\", count_of_rating_5, \"                   Percentage of Total=\", count_of_rating_5/df[\"rating\"].count()*100,\"%\")\n",
        "print(\"----------------------------\")\n",
        "\n",
        "#We can plot the number of ratings for each in a small graph here\n",
        "sns.countplot(x = 'rating', data = df)\n",
        "#(1-2)= negative\n",
        "#(4-5)= positive"
      ],
      "metadata": {
        "id": "-hfa6Jeh8_AC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## We group ratings (1-2) and (4,5) to show total count of negative and positive sentiment respectively"
      ],
      "metadata": {
        "id": "fGbIBb9kDbwd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#we create a new data frame called sentiment to have only positive and negative classes so we can compare safely without skewing the original data set\n",
        "df['Sentiment'] = df['rating'].apply(lambda rating : 'negative' if rating < 4 else 'positive')\n",
        "\n",
        "#we then plot the total count of each sentiment\n",
        "sns.countplot(x = 'Sentiment', data = df)\n",
        "\n",
        "negative_sentiment = (df['Sentiment'] == 'negative').sum()\n",
        "print(\"Count of 'negative sentiment':\", negative_sentiment, \"                   Percentage of Total=\", negative_sentiment/df[\"Sentiment\"].count()*100,\"%\")\n",
        "\n",
        "positive_sentiment = (df['Sentiment'] == 'positive').sum()\n",
        "print(\"Count of 'positive sentiment':\", positive_sentiment, \"                   Percentage of Total=\", positive_sentiment/df[\"Sentiment\"].count()*100,\"%\")\n",
        "\n",
        "#Data set appears to be prefectly balanced"
      ],
      "metadata": {
        "id": "W9JEImEmDZfI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##since we've covered the ratings we move on to the reviews"
      ],
      "metadata": {
        "id": "wZJS-0HCBo0s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install arabic-reshaper\n",
        "!pip install python-bidi\n",
        "\n",
        "from collections import Counter\n",
        "from arabic_reshaper import reshape\n",
        "from bidi.algorithm import get_display\n",
        "\n",
        "# Inspect the review column\n",
        "print(df['review'].describe())  # Basic statistics (count, unique, top, freq)\n",
        "print(\"\\nSample Reviews:\\n\", df['review'].sample(5))  # Print random 5 reviews\n",
        "\n",
        "\n",
        "\n",
        "!pip install emoji\n",
        "import re\n",
        "import emoji\n",
        "\n",
        "# Function to check if a review contains English letters\n",
        "def contains_english(text):\n",
        "    return bool(re.search(r'[a-zA-Z]', text))\n",
        "\n",
        "# Function to check if a review contains emojis\n",
        "def contains_emoji(text):\n",
        "    return any(char in emoji.EMOJI_DATA for char in text)\n",
        "\n",
        "# Apply functions to create new columns\n",
        "df[\"has_english\"] = df[\"review\"].apply(contains_english)\n",
        "df[\"has_emoji\"] = df[\"review\"].apply(contains_emoji)\n",
        "\n",
        "# Check if a review has both English & emoji\n",
        "df[\"has_both\"] = df[\"has_english\"] & df[\"has_emoji\"]\n",
        "\n",
        "num_english = df[\"has_english\"].sum()\n",
        "num_emoji = df[\"has_emoji\"].sum()\n",
        "num_both = df[\"has_both\"].sum()\n",
        "\n",
        "print(f\"Reviews with English: {num_english}\")\n",
        "print(f\"Reviews with Emojis: {num_emoji}\")\n",
        "print(f\"Reviews with Both English & Emojis: {num_both}\")\n",
        "\n",
        "\n",
        "print(\"\\nSample Reviews with English:\")\n",
        "print(df[df[\"has_english\"]][\"review\"].sample(5).tolist())\n",
        "\n",
        "print(\"\\nSample Reviews with Emojis:\")\n",
        "print(df[df[\"has_emoji\"]][\"review\"].sample(5).tolist())\n",
        "\n",
        "print(\"\\nSample Reviews with Both English & Emojis:\")\n",
        "print(df[df[\"has_both\"]][\"review\"].sample(5).tolist())\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Combine all reviews into a single string\n",
        "all_reviews = ' '.join(df['review'].astype(str))\n",
        "\n",
        "# Tokenization (splitting into words)\n",
        "words = all_reviews.split()\n",
        "\n",
        "# Count word frequencies\n",
        "word_counts = Counter(words)\n",
        "\n",
        "# Get the top 30 most frequent words\n",
        "top_30_words = word_counts.most_common(30)\n",
        "\n",
        "# Prepare Arabic words for visualization\n",
        "words, counts = zip(*top_30_words)\n",
        "reshaped_words = [reshape(word) for word in words]\n",
        "display_words = [get_display(word) for word in reshaped_words]\n",
        "\n",
        "# Plot word frequencies\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.bar(display_words, counts)\n",
        "plt.xlabel(\"Words\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "plt.title(\"Top 30 Most Frequent Words in Reviews\")\n",
        "plt.xticks(rotation=45, ha='right')  # Rotate x-axis labels for better readability\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# import seaborn as sns\n",
        "\n",
        "# # Compute review lengths\n",
        "# df['review_length'] = df['review'].astype(str).apply(len)\n",
        "\n",
        "# # Plot distribution\n",
        "# plt.figure(figsize=(10, 5))\n",
        "# sns.histplot(df['review_length'], bins=30, kde=True)\n",
        "# plt.xlabel(\"Review Length (Characters)\")\n",
        "# plt.ylabel(\"Frequency\")\n",
        "# plt.title(\"Distribution of Review Lengths\")\n",
        "# plt.show()\n",
        "\n",
        "\n",
        "\n",
        "# # Compute review length in words and characters\n",
        "# df['word_count'] = df['review'].astype(str).apply(lambda x: len(x.split()))\n",
        "# df['char_count'] = df['review'].astype(str).apply(len)\n",
        "\n",
        "# # Plot word count distribution\n",
        "# plt.figure(figsize=(10, 5))\n",
        "# sns.histplot(df['word_count'], bins=30, kde=True)\n",
        "# plt.xlabel(\"Number of Words in Review\")\n",
        "# plt.ylabel(\"Frequency\")\n",
        "# plt.title(\"Distribution of Word Count in Reviews\")\n",
        "# plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "vNi3n1snBvs6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "since we've inspected our main 2 attributes rating and review. we move on to data preperation where we drop other columns and start preprocessing the reviews"
      ],
      "metadata": {
        "id": "XnPGRebeScaZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Data preperation**"
      ],
      "metadata": {
        "id": "g01DoqudStuI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_copy = df[['rating', 'review']].copy()\n",
        "\n",
        "##to better see reviews\n",
        "pd.set_option('display.max_colwidth', None)  # Prevents text shrinking\n",
        "pd.set_option('display.width', 1000)  # Adjusts display width\n",
        "\n",
        "\n",
        "df_copy.head()"
      ],
      "metadata": {
        "id": "Q5UGXiYoS4VW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##We start with Normalizing the arabic text"
      ],
      "metadata": {
        "id": "4lZtOJ-AQ862"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "since english represents only 2.56% we keep it as is bec. tf-idf and automl tools can still\n",
        "recognise it without bias\n",
        "emojis are converted because they hold sentiment"
      ],
      "metadata": {
        "id": "Ct3hkZhan7jv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#To normalize arabic text we need to removing diacritics (tashkeel), removing elongation of words (tatweel) converting variations of the same certain letter to a standard unified form\n",
        "#couldnt use farasa so opted for camel tools\n",
        "\n",
        "# !pip install farasa # cant normalise with it :( docs arent available and cant sign up\n",
        "!pip install camel-tools\n",
        "import re\n",
        "from camel_tools.utils.normalize import normalize_alef_maksura_ar\n",
        "from camel_tools.utils.normalize import normalize_alef_ar\n",
        "from camel_tools.utils.normalize import normalize_teh_marbuta_ar\n",
        "from camel_tools.utils.normalize import normalize_unicode\n",
        "from camel_tools.utils.dediac import dediac_ar\n",
        "\n",
        "\n",
        "punctuation_pattern = re.compile(r\"[-،؟.!\\\"':;(){}“”‘’,.&+\\^\\*\\%@#/~=_\\[\\]<>|\\\\\\n\\t]\")# Remove Arabic & English punctuation\n",
        "quotes_pattern = re.compile(r'[\\\"\\'“”‘’]')  # Matches only quotation marks (Arabic & English)\n",
        "\n",
        "\n",
        "def remove_elongation(text):\n",
        "    # Rule 1: Remove if a letter is repeated 3+ times anywhere\n",
        "    text = re.sub(r'(.)\\1{2,}', r'\\1', text)\n",
        "\n",
        "    # Rule 2: Remove if a letter is repeated 2+ times at the end of the word\n",
        "    text = re.sub(r'(\\w)\\1$', r'\\1', text)\n",
        "\n",
        "    return text\n",
        "\n",
        "\n",
        "# Dictionary to store emoji conversions\n",
        "emoji_conversion_log = {}\n",
        "\n",
        "def convert_emojis_to_arabic(text):\n",
        "    converted_text = emoji.demojize(text, language='ar')  # 😍 → \":وجه_بعيون_على_شكل_قلوب:\"\n",
        "    converted_text_cleaned = converted_text.replace(\":\", \"\").replace(\"_\", \" \")  # :\"وجه بعيون على شكل قلوب\"\n",
        "\n",
        "    # Log changes if an emoji was actually converted\n",
        "    if text != converted_text:\n",
        "        emoji_conversion_log[text] = converted_text_cleaned\n",
        "\n",
        "    return converted_text_cleaned\n",
        "\n",
        "\n",
        "def preprocess_text(text):\n",
        "    text = normalize_unicode(text)  # Step 1: Normalize Unicode\n",
        "    text = remove_elongation(text)  # Step 2: Remove elongation ## removes momtaz and other normal arabic words a workaround could be 3+ letters but will try to find a library first\n",
        "    text = re.sub(r'[٠-٩]', lambda x: str(ord(x.group()) - ord('٠')), text)  # Step 3: Convert Arabic numbers to English\n",
        "    text = convert_emojis_to_arabic(text)\n",
        "    # text = re.sub(r'\\d+', '', text)  # Step 4: Remove all numbers ##for now i wont reemove cause\n",
        "    text = quotes_pattern.sub('', text)  # Step 5: Remove quotation marks (but keep text inside)\n",
        "    text = dediac_ar(text)  # Step 6: Remove diacritics\n",
        "    text = normalize_alef_maksura_ar(text)  # Step 7: Normalize ى → ي\n",
        "    text = normalize_teh_marbuta_ar(text)  # Step 8: Normalize ة → ه\n",
        "    text = normalize_alef_ar(text) # step 9  Normalize alef variants to 'ا'\n",
        "    text = punctuation_pattern.sub('', text)  # Step 10: Remove punctuation\n",
        "    # text = re.sub(r'(?<!\\w)و(?=\\w)', r'و ', text)  # Add space after و only if it's at the start\n",
        "    # text = re.sub(r'(?<=\\w)و(?!\\w)', r' و', text)  # Add space before و only if it's at the end # Ensure \"و\" is separated only when it's at the beginning of a word\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "    return text\n",
        "\n",
        "\n",
        "print()\n",
        "print(\"Before camel tools and manual normalization\")\n",
        "print(df_copy['review'].head(20))\n",
        "print(\"----------------------------\")\n",
        "print()\n",
        "\n",
        "#Normalization applied to entire DataFrame\n",
        "df['normalized_review'] = df['review'].astype(str).apply(preprocess_text)\n",
        "\n",
        "\n",
        "# Function to check if text still contains emojis\n",
        "def contains_emoji(text):\n",
        "    return any(char in emoji.EMOJI_DATA for char in text)\n",
        "\n",
        "#Check for emojis in the normalized reviews\n",
        "df[\"has_emoji\"] = df[\"normalized_review\"].apply(contains_emoji)\n",
        "\n",
        "#num of reviews with emojis\n",
        "num_reviews_with_emojis = df[\"has_emoji\"].sum()\n",
        "print(f\"\\n Num of reviews containing emojis AFTER normalization: {num_reviews_with_emojis}\")\n",
        "\n",
        "\n",
        "#sample of emoji conversions\n",
        "print(\"\\n sample of Emoji Conversions (First 10)\")\n",
        "for original, converted in list(emoji_conversion_log.items())[:10]:\n",
        "    print(f\"{original} → {converted}\")\n",
        "\n",
        "print(\"After camel tools and manual normalization\")\n",
        "print(df['normalized_review'].head(20))\n",
        "print(\"----------------------------\")\n",
        "print()"
      ],
      "metadata": {
        "id": "YdifI7cLRlYG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## After normalizing text we do Tokenization"
      ],
      "metadata": {
        "id": "PhqlHNYuuEDz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from camel_tools.tokenizers.word import simple_word_tokenize\n",
        "\n",
        "# Tokenize the normalized reviews\n",
        "df['tokens'] = df['normalized_review'].apply(lambda x: simple_word_tokenize(x))\n",
        "\n",
        "# Display the first few rows to verify tokenization\n",
        "print(df[['normalized_review', 'tokens']].head(20))\n"
      ],
      "metadata": {
        "id": "wf3IlhHbuTim"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **We then remove stop words**"
      ],
      "metadata": {
        "id": "gJK99giKx9HH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Mounting Google Drive again\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "\n",
        "# Specify the path to your stopwords file\n",
        "stopwords_file_path = \"/content/drive/MyDrive/thesisdata/updated_stopwords.txt\"\n",
        "\n",
        "# Load stopwords as a set\n",
        "df_stopwords = set(pd.read_csv(stopwords_file_path, header=None, encoding=\"utf-8\")[0].tolist())\n",
        "\n",
        "# Applying stopword removal\n",
        "df[\"filtered_tokens\"] = df[\"tokens\"].apply(lambda tokens: [word for word in tokens if word not in df_stopwords])\n",
        "\n",
        "\n",
        "from collections import Counter\n",
        "\n",
        "# Get all removed stopwords\n",
        "removed_words = []\n",
        "for original, filtered in zip(df[\"tokens\"], df[\"filtered_tokens\"]):\n",
        "    removed_words.extend([word for word in original if word not in filtered])\n",
        "\n",
        "# Count removed words\n",
        "removed_counts = Counter(removed_words)\n",
        "\n",
        "# Print top 200 most removed words\n",
        "print(\"Most removed stopwords:\", removed_counts.most_common(200))\n",
        "\n",
        "\n",
        "# Display results\n",
        "print(\"Tokens before filtering stop words\")\n",
        "print(df[\"tokens\"].head(10))\n",
        "print(\"---------------------\\n\")\n",
        "\n",
        "print(\"Tokens after filtering stop words\")\n",
        "print(df[\"filtered_tokens\"].head(10))\n",
        "print(\"----------------------\\n\")\n",
        "\n"
      ],
      "metadata": {
        "id": "nKAR-0DEyDyv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##After tokenization and removing stop words we whould now have a cleaned dataset. we can now proceed with lemmatization to bring words back to their root form"
      ],
      "metadata": {
        "id": "c91rCfh4inHz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install stanza\n",
        "# import stanza\n",
        "# stanza.download(\"ar\")  # This downloads the Arabic NLP model\n",
        "\n",
        "\n",
        "# #all needed cause stanza analyzes text by looking at the whole sentence to decide on the correct lemma so it group the sentence together decides then does tokenization again\n",
        "# nlp = stanza.Pipeline(lang=\"ar\", processors=\"tokenize,mwt,pos,lemma\")\n",
        "\n",
        "# #Lemmatize Tokenized Words\n",
        "# def lemmatize_tokens(tokens):\n",
        "#     text = \" \".join(tokens)  # Convert list of tokens to a single text string to judge first\n",
        "#     doc = nlp(text)\n",
        "#     return [word.lemma for sent in doc.sentences for word in sent.words]\n",
        "\n",
        "# #applying lemmatization\n",
        "# df[\"lemmatized_tokens\"] = df[\"filtered_tokens\"].apply(lemmatize_tokens)\n",
        "\n",
        "# #sample results\n",
        "# print(df[[\"filtered_tokens\", \"lemmatized_tokens\"]].head(10))\n",
        "\n",
        "# #copy of lemmatized reviews\n",
        "# df.to_csv(\"lemmatized_reviews.csv\", encoding=\"utf-16\", index=False)\n",
        "\n"
      ],
      "metadata": {
        "id": "8YcA_-_gizaT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##After lemmatization we can now proceed to word embeddings so that ML models understand words. here we use **TF-IDF**"
      ],
      "metadata": {
        "id": "95nbM5IsizwP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# print(\"before lemmatization\")\n",
        "# print(df['filtered_tokens'].head(30))\n",
        "# print(\"----------------------------\")\n",
        "# print()\n",
        "\n",
        "\n",
        "# print(\"after lemmatization\")\n",
        "# df['lemmatized_tokens'].head(30)"
      ],
      "metadata": {
        "id": "ZBbBIHvfi_ML"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import pandas as pd\n",
        "# import random\n",
        "# from collections import Counter\n",
        "# from nltk.stem.isri import ISRIStemmer\n",
        "# from nltk.stem.snowball import ArabicStemmer\n",
        "\n",
        "# # ✅ Initialize Stemmers\n",
        "# isri_stemmer = ISRIStemmer()\n",
        "# snowball_stemmer = ArabicStemmer()\n",
        "\n",
        "# # ✅ Apply Stemming to Each Token List\n",
        "# df[\"isri_stemmed_tokens\"] = df[\"filtered_tokens\"].apply(lambda tokens: [isri_stemmer.stem(word) for word in tokens])\n",
        "# df[\"snowball_stemmed_tokens\"] = df[\"filtered_tokens\"].apply(lambda tokens: [snowball_stemmer.stem(word) for word in tokens])\n",
        "\n",
        "# # ✅ Function to Evaluate Stemming Effectiveness\n",
        "# def evaluate_stemming(df, original_column, stemmed_column, stemmer_name):\n",
        "#     original_tokens = df[original_column].explode().dropna()\n",
        "#     stemmed_tokens = df[stemmed_column].explode().dropna()\n",
        "\n",
        "#     # 📌 Count Unique Words Before & After\n",
        "#     unique_original = set(original_tokens)\n",
        "#     unique_stemmed = set(stemmed_tokens)\n",
        "\n",
        "#     print(f\"🔹 {stemmer_name} Stemmer Evaluation 🔹\")\n",
        "#     print(f\"📌 Unique Words Before: {len(unique_original)}\")\n",
        "#     print(f\"📌 Unique Words After Stemming: {len(unique_stemmed)}\")\n",
        "\n",
        "#     # 🔍 Count How Many Words Changed\n",
        "#     changed_words = [o for o, s in zip(original_tokens, stemmed_tokens) if o != s]\n",
        "#     print(f\"🔍 Words That Changed: {len(set(changed_words))} / {len(unique_original)} ({(len(set(changed_words)) / len(unique_original)) * 100:.2f}%)\")\n",
        "\n",
        "#     # ⚠️ Check for Inconsistencies (same word, different stems)\n",
        "#     stem_counts = Counter(stemmed_tokens)\n",
        "#     inconsistent_words = {word for word, count in stem_counts.items() if count > 1}\n",
        "#     print(f\"⚠️ Inconsistently Stemmed Words: {len(inconsistent_words)}\\n\")\n",
        "\n",
        "#     return changed_words, inconsistent_words\n",
        "\n",
        "# # ✅ Run Evaluation for Both Stemmers\n",
        "# changed_isri, inconsistent_isri = evaluate_stemming(df, \"filtered_tokens\", \"isri_stemmed_tokens\", \"ISRI\")\n",
        "# changed_snowball, inconsistent_snowball = evaluate_stemming(df, \"filtered_tokens\", \"snowball_stemmed_tokens\", \"Snowball\")\n",
        "\n",
        "# # ✅ Show Random 5 Samples Before & After Stemming\n",
        "# samples = random.sample(range(len(df)), 5)\n",
        "# print(\"🔍 Sample Stemming Comparison 🔍\\n\")\n",
        "# for i in samples:\n",
        "#     print(f\"🔵 Original: {df['filtered_tokens'].iloc[i]}\")\n",
        "#     print(f\"🟢 ISRI Stemmed: {df['isri_stemmed_tokens'].iloc[i]}\")\n",
        "#     print(f\"🟣 Snowball Stemmed: {df['snowball_stemmed_tokens'].iloc[i]}\")\n",
        "#     print(\"-\" * 100)\n"
      ],
      "metadata": {
        "id": "3aK0peOvghCf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ✅ Convert tokenized words into a string for each review\n",
        "df[\"filtered_text\"] = df[\"filtered_tokens\"].apply(lambda tokens: \" \".join(tokens))\n",
        "\n",
        "# 🔍 Display a sample\n",
        "print(df[[\"filtered_tokens\", \"filtered_text\"]].head(5))\n",
        "\n",
        "# ✅ Apply TF-IDF Vectorization\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import pandas as pd\n",
        "\n",
        "# Initialize TF-IDF Vectorizer (Limit to top 15000 words)\n",
        "tfidf_vectorizer = TfidfVectorizer(max_features=15000)\n",
        "\n",
        "# Transform text into TF-IDF representation\n",
        "X_tfidf = tfidf_vectorizer.fit_transform(df[\"filtered_text\"])\n",
        "\n",
        "# Convert TF-IDF to DataFrame\n",
        "X_tfidf_df = pd.DataFrame(X_tfidf.toarray(), columns=tfidf_vectorizer.get_feature_names_out())\n",
        "\n",
        "# 🔍 Display shape of TF-IDF matrix\n",
        "print(f\"✅ TF-IDF Matrix Shape: {X_tfidf.shape}\")\n",
        "print(X_tfidf_df.head(5))\n",
        "\n",
        "# ✅ Ensure df has 'rating' column before merging\n",
        "df_copy = df[['rating']].copy()  # Keeping only relevant column\n",
        "\n",
        "# ✅ Merge TF-IDF features with ratings\n",
        "df_tfidf_final = pd.concat([df_copy, X_tfidf_df], axis=1)\n",
        "\n",
        "# 🔍 Check the merged dataset\n",
        "print(df_tfidf_final.head(5))\n",
        "print(f\"✅ Final dataset shape: {df_tfidf_final.shape}\")\n",
        "\n",
        "# ✅ Save TF-IDF Vectorizer and Features\n",
        "import pickle\n",
        "\n",
        "# Save the TF-IDF vectorizer for future use\n",
        "with open(\"tfidf_vectorizer.pkl\", \"wb\") as f:\n",
        "    pickle.dump(tfidf_vectorizer, f)\n",
        "\n",
        "# Save transformed TF-IDF features + ratings\n",
        "df_tfidf_final.to_csv(\"tfidf_with_ratings.csv\", index=False, encoding=\"utf-16\")\n",
        "\n",
        "print(\"✅ TF-IDF features with ratings saved successfully!\")\n"
      ],
      "metadata": {
        "id": "J2Et3nbNs9bK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##We now have our reviews in a numerical form and ready for modelling"
      ],
      "metadata": {
        "id": "MQO7_va-PPm-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ✅ Ensure required libraries are installed\n",
        "!pip install tpot scikit-learn pandas numpy\n",
        "\n",
        "# ✅ Import necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tpot import TPOTClassifier\n",
        "import pickle\n",
        "\n",
        "# ✅ Load TF-IDF dataset (ensures it's available if running separately)\n",
        "try:\n",
        "    df_tfidf_final = pd.read_csv(\"tfidf_with_ratings.csv\", encoding=\"utf-16\")\n",
        "    print(\"✅ TF-IDF dataset loaded successfully!\")\n",
        "except FileNotFoundError:\n",
        "    print(\"❌ ERROR: TF-IDF dataset not found. Ensure Part 1 has been run.\")\n",
        "\n",
        "# ✅ Check dataset structure\n",
        "print(df_tfidf_final.head())\n",
        "print(f\"Dataset Shape: {df_tfidf_final.shape}\")\n",
        "\n",
        "# ✅ Separate features (TF-IDF vectors) and target (rating)\n",
        "X = df_tfidf_final.drop(columns=[\"rating\"])  # Features\n",
        "y = df_tfidf_final[\"rating\"]  # Target labels\n",
        "\n",
        "# ✅ Convert rating into a **binary sentiment** (Positive = 1, Negative = 0)\n",
        "y = y.apply(lambda x: 1 if x >= 4 else 0)  # Change threshold if needed\n",
        "\n",
        "# ✅ Split data into training & testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "print(f\"Training Size: {X_train.shape}, Testing Size: {X_test.shape}\")\n",
        "\n",
        "# ✅ Initialize TPOT Classifier\n",
        "tpot = TPOTClassifier(\n",
        "    generations=10,  # Increase for better optimization (Default: 5)\n",
        "    population_size=50,  # More models per generation (Default: 20)\n",
        "    verbosity=2,  # Show progress details\n",
        "    n_jobs=-1,  # Use all available CPU cores\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# ✅ Train TPOT to find the best model\n",
        "tpot.fit(X_train, y_train)\n",
        "\n",
        "# ✅ Evaluate TPOT on test data\n",
        "accuracy = tpot.score(X_test, y_test)\n",
        "print(f\"✅ TPOT Best Model Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "# ✅ Export the best model as a Python script\n",
        "tpot.export(\"best_tpot_pipeline.py\")\n",
        "\n",
        "# ✅ Save the trained best model for later use\n",
        "with open(\"tpot_best_model.pkl\", \"wb\") as f:\n",
        "    pickle.dump(tpot.fitted_pipeline_, f)\n",
        "\n",
        "print(\"✅ TPOT best model saved successfully!\")\n",
        "\n",
        "# ===========================\n",
        "# 🔍 **Load & Use the Model**\n",
        "# ===========================\n",
        "\n",
        "# # Load the trained TPOT model\n",
        "# with open(\"tpot_best_model.pkl\", \"rb\") as f:\n",
        "#     loaded_model = pickle.load(f)\n",
        "\n",
        "# # Predict on test data\n",
        "# predictions = loaded_model.predict(X_test)\n",
        "# print(predictions[:10])  # Show sample predictions\n"
      ],
      "metadata": {
        "id": "xF0y8cOQPYLr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}