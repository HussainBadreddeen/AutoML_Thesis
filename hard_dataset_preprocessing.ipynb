{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "https://github.com/HussainBadreddeen/AutoML_Thesis/blob/main/hard_dataset_preprocessing.ipynb",
      "authorship_tag": "ABX9TyMXfgZEbiBJXQEg1j4dabVX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HussainBadreddeen/AutoML_Thesis/blob/main/hard_dataset_preprocessing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Importing libraries needed and importing the dataset from google drive"
      ],
      "metadata": {
        "id": "54Ts7bN441C4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NGVZQ1rp-Q2m"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "%matplotlib inline\n",
        "sns.set()\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "\n",
        "\n",
        "file_path = \"/content/drive/MyDrive/thesisdata/balanced-reviews.txt\"\n",
        "\n",
        "# Load as a DataFrame (change delimiter if needed)\n",
        "df = pd.read_csv(file_path, sep=\"\\t\", encoding=\"utf-16\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## I follow the CRISP-DM process here.\n",
        "I've already done business understanding since I've mentioned the goal and objectives of this project is to compare AutoML for arabic sentiment analysis with manual tuning of hyperparameters"
      ],
      "metadata": {
        "id": "3CVj2ipW6W6W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Data Understanding**"
      ],
      "metadata": {
        "id": "PfnWQ11w5iC4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#first few rows\n",
        "print(df.head())\n",
        "print(\"----------------------------\")\n",
        "print()\n",
        "#Number of Rows and coloumns\n",
        "print('Number of rows and columns in the data set:', df.shape)\n",
        "print(\"----------------------------\")\n",
        "print()\n",
        "#Data types of the dataset\n",
        "df.info()\n",
        "print(\"----------------------------\")\n",
        "print()\n",
        "\n",
        "# Display summary statistics for numerical features\n",
        "print(df.describe())\n",
        "print(\"----------------------------\")\n",
        "print()\n",
        "#quick check if there are any missing values\n",
        "print(\"Missing Values:\")\n",
        "print(df.isnull().sum())\n",
        "print(\"----------------------------\")\n",
        "print()"
      ],
      "metadata": {
        "id": "8H5Z2T_aHe8b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##since we know that we are trying to measure sentiment we focus on data related to reviews and their ratings"
      ],
      "metadata": {
        "id": "ajGxzFiC8zld"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Checking How many values are Unqiue from the total values for the Column rating\n",
        "print(\"Amount of Values in the column:\", df[\"rating\"].count())\n",
        "print(\"----------------------------\")\n",
        "unique_ratings = df['rating'].nunique()\n",
        "print(f\"Number of unique ratings: {unique_ratings}\")\n",
        "print(\"The unique values are: \" ,df['rating'].unique())\n",
        "#1,2,4,5 are the unique values as they represent either negative (1,2) or positive (4,5). 3 was removed in the balanced version of the dataset\n",
        "print(\"----------------------------\")\n",
        "\n",
        "count_of_rating_1 = (df['rating'] == 1).sum()\n",
        "print(\"Count of 'rating one':\", count_of_rating_1, \"                   Percentage of Total=\", count_of_rating_1/df[\"rating\"].count()*100,\"%\")\n",
        "print(\"----------------------------\")\n",
        "\n",
        "count_of_rating_2 = (df['rating'] == 2).sum()\n",
        "print(\"Count of 'rating two':\", count_of_rating_2, \"                   Percentage of Total=\", count_of_rating_2/df[\"rating\"].count()*100,\"%\")\n",
        "print(\"----------------------------\")\n",
        "\n",
        "count_of_rating_4 = (df['rating'] == 4).sum()\n",
        "print(\"Count of 'rating four':\", count_of_rating_4, \"                   Percentage of Total=\", count_of_rating_4/df[\"rating\"].count()*100,\"%\")\n",
        "print(\"----------------------------\")\n",
        "\n",
        "count_of_rating_5 = (df['rating'] == 5).sum()\n",
        "print(\"Count of 'rating five':\", count_of_rating_5, \"                   Percentage of Total=\", count_of_rating_5/df[\"rating\"].count()*100,\"%\")\n",
        "print(\"----------------------------\")\n",
        "\n",
        "#We can plot the number of ratings for each in a small graph here\n",
        "sns.countplot(x = 'rating', data = df)\n",
        "#(1-2)= negative\n",
        "#(4-5)= positive"
      ],
      "metadata": {
        "id": "-hfa6Jeh8_AC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## We group ratings (1-2) and (4,5) to show total count of negative and positive sentiment respectively"
      ],
      "metadata": {
        "id": "fGbIBb9kDbwd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#we create a new data frame called sentiment to have only positive and negative classes so we can compare safely without skewing the original data set\n",
        "df['Sentiment'] = df['rating'].apply(lambda rating : 'negative' if rating < 4 else 'positive')\n",
        "\n",
        "#we then plot the total count of each sentiment\n",
        "sns.countplot(x = 'Sentiment', data = df)\n",
        "\n",
        "negative_sentiment = (df['Sentiment'] == 'negative').sum()\n",
        "print(\"Count of 'negative sentiment':\", negative_sentiment, \"                   Percentage of Total=\", negative_sentiment/df[\"Sentiment\"].count()*100,\"%\")\n",
        "\n",
        "positive_sentiment = (df['Sentiment'] == 'positive').sum()\n",
        "print(\"Count of 'positive sentiment':\", positive_sentiment, \"                   Percentage of Total=\", positive_sentiment/df[\"Sentiment\"].count()*100,\"%\")\n",
        "\n",
        "#Data set appears to be prefectly balanced"
      ],
      "metadata": {
        "id": "W9JEImEmDZfI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##since we've covered the ratings we move on to the reviews"
      ],
      "metadata": {
        "id": "wZJS-0HCBo0s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install arabic-reshaper\n",
        "!pip install python-bidi\n",
        "\n",
        "from collections import Counter\n",
        "from arabic_reshaper import reshape\n",
        "from bidi.algorithm import get_display\n",
        "\n",
        "# Inspect the review column\n",
        "print(df['review'].describe())  # Basic statistics (count, unique, top, freq)\n",
        "print(\"\\nSample Reviews:\\n\", df['review'].sample(5))  # Print random 5 reviews\n",
        "\n",
        "\n",
        "\n",
        "!pip install emoji\n",
        "import re\n",
        "import emoji\n",
        "\n",
        "# Function to check if a review contains English letters\n",
        "def contains_english(text):\n",
        "    return bool(re.search(r'[a-zA-Z]', text))\n",
        "\n",
        "# Function to check if a review contains emojis\n",
        "def contains_emoji(text):\n",
        "    return any(char in emoji.EMOJI_DATA for char in text)\n",
        "\n",
        "# Apply functions to create new columns\n",
        "df[\"has_english\"] = df[\"review\"].apply(contains_english)\n",
        "df[\"has_emoji\"] = df[\"review\"].apply(contains_emoji)\n",
        "\n",
        "# Check if a review has both English & emoji\n",
        "df[\"has_both\"] = df[\"has_english\"] & df[\"has_emoji\"]\n",
        "\n",
        "num_english = df[\"has_english\"].sum()\n",
        "num_emoji = df[\"has_emoji\"].sum()\n",
        "num_both = df[\"has_both\"].sum()\n",
        "\n",
        "print(f\"Reviews with English: {num_english}\")\n",
        "print(f\"Reviews with Emojis: {num_emoji}\")\n",
        "print(f\"Reviews with Both English & Emojis: {num_both}\")\n",
        "\n",
        "\n",
        "print(\"\\nSample Reviews with English:\")\n",
        "print(df[df[\"has_english\"]][\"review\"].sample(5).tolist())\n",
        "\n",
        "print(\"\\nSample Reviews with Emojis:\")\n",
        "print(df[df[\"has_emoji\"]][\"review\"].sample(5).tolist())\n",
        "\n",
        "print(\"\\nSample Reviews with Both English & Emojis:\")\n",
        "print(df[df[\"has_both\"]][\"review\"].sample(5).tolist())\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Combine all reviews into a single string\n",
        "all_reviews = ' '.join(df['review'].astype(str))\n",
        "\n",
        "# Tokenization (splitting into words)\n",
        "words = all_reviews.split()\n",
        "\n",
        "# Count word frequencies\n",
        "word_counts = Counter(words)\n",
        "\n",
        "# Get the top 100 most frequent words\n",
        "top_100_words = word_counts.most_common(100)\n",
        "\n",
        "# Prepare Arabic words for visualization\n",
        "words, counts = zip(*top_100_words)\n",
        "reshaped_words = [reshape(word) for word in words]\n",
        "display_words = [get_display(word) for word in reshaped_words]\n",
        "\n",
        "# Plot word frequencies\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.bar(display_words, counts)\n",
        "plt.xlabel(\"Words\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "plt.title(\"Top 100 Most Frequent Words in Reviews\")\n",
        "plt.xticks(rotation=45, ha='right')  # Rotate x-axis labels for better readability\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "vNi3n1snBvs6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "since we've inspected our main 2 attributes rating and review. we move on to data preperation where we drop other columns and start preprocessing the reviews"
      ],
      "metadata": {
        "id": "XnPGRebeScaZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Data preperation**"
      ],
      "metadata": {
        "id": "g01DoqudStuI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_copy = df[['rating', 'review']].copy()\n",
        "\n",
        "##to better see reviews\n",
        "pd.set_option('display.max_colwidth', None)  # Prevents text shrinking\n",
        "pd.set_option('display.width', 1000)  # Adjusts display width\n",
        "\n",
        "\n",
        "df_copy.head()"
      ],
      "metadata": {
        "id": "Q5UGXiYoS4VW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##We start with Normalizing the arabic text"
      ],
      "metadata": {
        "id": "4lZtOJ-AQ862"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "since english represents only 2.56% we keep it as is bec. tf-idf and automl tools can still\n",
        "recognise it without bias\n",
        "emojis are converted because they hold sentiment"
      ],
      "metadata": {
        "id": "Ct3hkZhan7jv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#To normalize arabic text we need to removing diacritics (tashkeel), removing elongation of words (tatweel) converting variations of the same certain letter to a standard unified form\n",
        "#couldnt use farasa so opted for camel tools\n",
        "\n",
        "# !pip install farasa # cant normalise with it :( docs arent available and cant sign up\n",
        "!pip install camel-tools\n",
        "import re\n",
        "from camel_tools.utils.normalize import normalize_alef_maksura_ar\n",
        "from camel_tools.utils.normalize import normalize_alef_ar\n",
        "from camel_tools.utils.normalize import normalize_teh_marbuta_ar\n",
        "from camel_tools.utils.normalize import normalize_unicode\n",
        "from camel_tools.utils.dediac import dediac_ar\n",
        "\n",
        "\n",
        "punctuation_pattern = re.compile(r\"[-ÿåÿü.!\\\"':;(){}‚Äú‚Äù‚Äò‚Äô,.&+\\^\\*\\%@#/~=_\\[\\]<>|\\\\\\n\\t]\")# Remove Arabic & English punctuation\n",
        "quotes_pattern = re.compile(r'[\\\"\\'‚Äú‚Äù‚Äò‚Äô]')  # Matches only quotation marks (Arabic & English)\n",
        "\n",
        "\n",
        "def remove_elongation(text):\n",
        "    # Rule 1: Remove if a letter is repeated 3+ times anywhere\n",
        "    text = re.sub(r'(.)\\1{2,}', r'\\1', text)\n",
        "\n",
        "    # Rule 2: Remove if a letter is repeated 2+ times at the end of the word\n",
        "    text = re.sub(r'(\\w)\\1$', r'\\1', text)\n",
        "\n",
        "    return text\n",
        "\n",
        "\n",
        "# Dictionary to store emoji conversions\n",
        "emoji_conversion_log = {}\n",
        "\n",
        "def convert_emojis_to_arabic(text):\n",
        "    converted_text = emoji.demojize(text, language='ar')  # üòç ‚Üí \":Ÿàÿ¨Ÿá_ÿ®ÿπŸäŸàŸÜ_ÿπŸÑŸâ_ÿ¥ŸÉŸÑ_ŸÇŸÑŸàÿ®:\"\n",
        "    converted_text_cleaned = converted_text.replace(\":\", \"\").replace(\"_\", \" \")  # :\"Ÿàÿ¨Ÿá ÿ®ÿπŸäŸàŸÜ ÿπŸÑŸâ ÿ¥ŸÉŸÑ ŸÇŸÑŸàÿ®\"\n",
        "\n",
        "    # Log changes if an emoji was actually converted\n",
        "    if text != converted_text:\n",
        "        emoji_conversion_log[text] = converted_text_cleaned\n",
        "\n",
        "    return converted_text_cleaned\n",
        "\n",
        "\n",
        "def preprocess_text(text):\n",
        "    text = normalize_unicode(text)  # Step 1: Normalize Unicode\n",
        "    text = remove_elongation(text)  # Step 2: Remove elongation ## removes momtaz and other normal arabic words a workaround could be 3+ letters but will try to find a library first. update: didnt find but this seems to be working fine\n",
        "    text = re.sub(r'[Ÿ†-Ÿ©]', lambda x: str(ord(x.group()) - ord('Ÿ†')), text)  # Step 3: Convert Arabic numbers to English\n",
        "    text = convert_emojis_to_arabic(text)\n",
        "    text = re.sub(r'\\d+', '', text)  # Step 4: Remove all numbers ##for now i wont reemove cause\n",
        "    text = quotes_pattern.sub('', text)  # Step 5: Remove quotation marks (but keep text inside)\n",
        "    text = dediac_ar(text)  # Step 6: Remove diacritics\n",
        "    text = normalize_alef_maksura_ar(text)  # Step 7: Normalize Ÿâ ‚Üí Ÿä\n",
        "    text = normalize_teh_marbuta_ar(text)  # Step 8: Normalize ÿ© ‚Üí Ÿá\n",
        "    text = normalize_alef_ar(text) # step 9  Normalize alef variants to 'ÿß'\n",
        "    text = punctuation_pattern.sub('', text)  # Step 10: Remove punctuation\n",
        "    # text = re.sub(r'(?<!\\w)Ÿà(?=\\w)', r'Ÿà ', text)  # Add space after Ÿà only if it's at the start\n",
        "    # text = re.sub(r'(?<=\\w)Ÿà(?!\\w)', r' Ÿà', text)  # Add space before Ÿà only if it's at the end # Ensure \"Ÿà\" is separated only when it's at the beginning of a word\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "    return text\n",
        "\n",
        "\n",
        "print()\n",
        "print(\"Before camel tools and manual normalization\")\n",
        "print(df_copy['review'].head(20))\n",
        "print(\"----------------------------\")\n",
        "print()\n",
        "\n",
        "#Normalization applied to entire DataFrame\n",
        "df['normalized_review'] = df['review'].astype(str).apply(preprocess_text)\n",
        "\n",
        "\n",
        "# Function to check if text still contains emojis\n",
        "def contains_emoji(text):\n",
        "    return any(char in emoji.EMOJI_DATA for char in text)\n",
        "\n",
        "#Check for emojis in the normalized reviews\n",
        "df[\"has_emoji\"] = df[\"normalized_review\"].apply(contains_emoji)\n",
        "\n",
        "#num of reviews with emojis\n",
        "num_reviews_with_emojis = df[\"has_emoji\"].sum()\n",
        "print(f\"\\n Num of reviews containing emojis AFTER normalization: {num_reviews_with_emojis}\")\n",
        "\n",
        "\n",
        "#sample of emoji conversions\n",
        "print(\"\\n sample of Emoji Conversions (First 10)\")\n",
        "for original, converted in list(emoji_conversion_log.items())[:10]:\n",
        "    print(f\"{original} ‚Üí {converted}\")\n",
        "\n",
        "print(\"After camel tools and manual normalization\")\n",
        "print(df['normalized_review'].head(20))\n",
        "print(\"----------------------------\")\n",
        "print()"
      ],
      "metadata": {
        "id": "YdifI7cLRlYG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## After normalizing text we do Tokenization"
      ],
      "metadata": {
        "id": "PhqlHNYuuEDz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from camel_tools.tokenizers.word import simple_word_tokenize\n",
        "\n",
        "# Tokenize the normalized reviews\n",
        "df['tokens'] = df['normalized_review'].apply(lambda x: simple_word_tokenize(x))\n",
        "\n",
        "# Display the first few rows to verify tokenization\n",
        "print(df[['normalized_review', 'tokens']].head(20))\n"
      ],
      "metadata": {
        "id": "wf3IlhHbuTim"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **We then remove stop words**"
      ],
      "metadata": {
        "id": "gJK99giKx9HH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ‚úÖ Mount Google Drive for accessing stopwords file\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from collections import Counter\n",
        "\n",
        "#eng stop wrods\n",
        "nltk.download(\"stopwords\")\n",
        "\n",
        "#loading\n",
        "nltk_stopwords = set(stopwords.words(\"english\"))\n",
        "\n",
        "# loading the arabic stopwords customized list\n",
        "stopwords_file_path = \"/content/drive/MyDrive/thesisdata/updated_stopwords.txt\"\n",
        "custom_stopwords = set(pd.read_csv(stopwords_file_path, header=None, encoding=\"utf-8\")[0].tolist())\n",
        "\n",
        "# combing both stopwords lists\n",
        "all_stopwords = nltk_stopwords.union(custom_stopwords)\n",
        "\n",
        "# applying stopwords removal from normalized tezxt\n",
        "df[\"filtered_tokens\"] = df[\"tokens\"].apply(lambda tokens: [word for word in tokens if word.lower() not in all_stopwords])\n",
        "\n",
        "#Counting removed stopwords\n",
        "removed_words = []\n",
        "for original, filtered in zip(df[\"tokens\"], df[\"filtered_tokens\"]):\n",
        "    removed_words.extend([word for word in original if word.lower() in all_stopwords])\n",
        "\n",
        "removed_counts = Counter(removed_words)\n",
        "\n",
        "#showing top 200 most removed words\n",
        "print(\"Most removed stopwords:\", removed_counts.most_common(200))\n",
        "\n",
        "# ‚úÖ Display sample results # chatgpt\n",
        "print(\"Tokens before filtering stopwords:\\n\", df[\"tokens\"].head(10))\n",
        "print(\"\\nTokens after filtering stopwords:\\n\", df[\"filtered_tokens\"].head(10))\n"
      ],
      "metadata": {
        "id": "nKAR-0DEyDyv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##After tokenization and removing stop words we whould now have a cleaned dataset. we can now proceed with lemmatization to bring words back to their root form.\n",
        "\n",
        "#we try different tools:"
      ],
      "metadata": {
        "id": "c91rCfh4inHz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#1-stranza by stanford:"
      ],
      "metadata": {
        "id": "84TVLPHmKc6L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install stanza\n",
        "import stanza\n",
        "import pandas as pd\n",
        "from collections import Counter\n",
        "from camel_tools.utils.dediac import dediac_ar  # from camel tools before fa msh ha install again\n",
        "\n",
        "# ‚úÖ Load Stanza NLP Pipeline for Arabic (only necessary processors)\n",
        "stanza.download(\"ar\")\n",
        "nlp = stanza.Pipeline(\"ar\", processors=\"tokenize,lemma\")\n",
        "\n",
        "def lemmatize_arabic(text):\n",
        "    try:\n",
        "        doc = nlp(text)\n",
        "        lemmas = [word.lemma for sent in doc.sentences for word in sent.words]\n",
        "        return [dediac_ar(lemma) for lemma in lemmas]  # ‚úÖ Remove diacritics\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è Stanza Lemmatization Failed: {e}\")\n",
        "        return []\n",
        "\n",
        "df[\"stanza_lemmatized\"] = df[\"filtered_tokens\"].apply(lambda tokens: lemmatize_arabic(\" \".join(tokens)))\n",
        "\n",
        "# ‚úÖ Compute Evaluation Metrics\n",
        "def evaluate_lemmatization(original_tokens, lemmatized_tokens, stemmer_name=\"Stanza\"):\n",
        "    unique_original = set(original_tokens)\n",
        "    unique_stemmed = set(lemmatized_tokens)\n",
        "\n",
        "    print(f\"üîπ {stemmer_name} Lemmatizer Evaluation üîπ\")\n",
        "    print(f\"üìå Unique Words Before: {len(unique_original)}\")\n",
        "    print(f\"üìå Unique Words After Lemmatization: {len(unique_stemmed)}\")\n",
        "\n",
        "    # üîç Count How Many Words Changed\n",
        "    changed_words = [o for o, s in zip(original_tokens, lemmatized_tokens) if o != s]\n",
        "    print(f\"üîç Words That Changed: {len(set(changed_words))} / {len(unique_original)} ({(len(set(changed_words)) / len(unique_original)) * 100:.2f}%)\")\n",
        "\n",
        "    # ‚ö†Ô∏è Check for Inconsistencies (same word, different lemmas)\n",
        "    stem_counts = Counter(lemmatized_tokens)\n",
        "    inconsistent_words = {word for word, count in stem_counts.items() if count > 1}\n",
        "    print(f\"‚ö†Ô∏è Inconsistently Lemmatized Words: {len(inconsistent_words)}\\n\")\n",
        "\n",
        "    # üîπ Reduction Rate\n",
        "    reduction_rate = ((len(unique_original) - len(unique_stemmed)) / len(unique_original)) * 100\n",
        "\n",
        "    # üîπ Consistency Score\n",
        "    consistency_score = sum(1 for token in original_tokens if lemmatized_tokens.count(token) > 1) / len(original_tokens) * 100\n",
        "\n",
        "    return reduction_rate, consistency_score\n",
        "\n",
        "# ‚úÖ Apply Evaluation\n",
        "df[\"reduction_rate\"], df[\"consistency_score\"] = zip(*df.apply(\n",
        "    lambda row: evaluate_lemmatization(row[\"filtered_tokens\"], row[\"stanza_lemmatized\"]), axis=1\n",
        "))\n",
        "\n",
        "# üîç Display Average Results\n",
        "print(\"üìä Average Reduction Rate:\", df[\"reduction_rate\"].mean(), \"%\")\n",
        "print(\"üìä Average Consistency Score:\", df[\"consistency_score\"].mean(), \"%\")\n"
      ],
      "metadata": {
        "id": "8YcA_-_gizaT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#we try different approaches including\n",
        "##2-Tashphyne and 3-Khoja stemmer"
      ],
      "metadata": {
        "id": "tv51ixcRLW6H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ‚úÖ Install Required Libraries\n",
        "!pip install tashaphyne nltk\n",
        "\n",
        "from tashaphyne.stemming import ArabicLightStemmer\n",
        "from nltk.stem.isri import ISRIStemmer\n",
        "import pandas as pd\n",
        "from collections import Counter\n",
        "\n",
        "# ‚úÖ Initialize Tashaphyne Stemmer\n",
        "try:\n",
        "    tashaphyne_stemmer = ArabicLightStemmer()\n",
        "    tashaphyne_available = True\n",
        "except Exception as e:\n",
        "    print(f\"‚ö†Ô∏è Tashaphyne Initialization Failed: {e}\")\n",
        "    tashaphyne_available = False\n",
        "\n",
        "# ‚úÖ Initialize Khoja Stemmer\n",
        "try:\n",
        "    khoja_stemmer = ISRIStemmer()\n",
        "    khoja_available = True\n",
        "except Exception as e:\n",
        "    print(f\"‚ö†Ô∏è Khoja Initialization Failed: {e}\")\n",
        "    khoja_available = False\n",
        "\n",
        "# ‚úÖ Function to Apply Tashaphyne Stemming\n",
        "def tashaphyne_stem(tokens):\n",
        "    try:\n",
        "        return [tashaphyne_stemmer.light_stem(token) for token in tokens]\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è Tashaphyne Stemming Error: {e}\")\n",
        "        return tokens  # Return original tokens if error occurs\n",
        "\n",
        "# ‚úÖ Function to Apply Khoja Stemming\n",
        "def khoja_stem(tokens):\n",
        "    try:\n",
        "        return [khoja_stemmer.stem(token) for token in tokens]\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è Khoja Stemming Error: {e}\")\n",
        "        return tokens  # Return original tokens if error occurs\n",
        "\n",
        "# ‚úÖ Apply Stemming Methods\n",
        "if tashaphyne_available:\n",
        "    try:\n",
        "        df[\"tashaphyne_stemmed\"] = df[\"filtered_tokens\"].apply(lambda tokens: tashaphyne_stem(tokens))\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è Tashaphyne Stemming Failed: {e}\")\n",
        "        df[\"tashaphyne_stemmed\"] = df[\"filtered_tokens\"]\n",
        "\n",
        "if khoja_available:\n",
        "    try:\n",
        "        df[\"khoja_stemmed\"] = df[\"filtered_tokens\"].apply(lambda tokens: khoja_stem(tokens))\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è Khoja Stemming Failed: {e}\")\n",
        "        df[\"khoja_stemmed\"] = df[\"filtered_tokens\"]\n",
        "\n",
        "# ‚úÖ Evaluation Function\n",
        "def evaluate_stemming(original_tokens, stemmed_tokens, stemmer_name):\n",
        "    unique_original = set(original_tokens)\n",
        "    unique_stemmed = set(stemmed_tokens)\n",
        "\n",
        "    print(f\"üîπ {stemmer_name} Stemmer Evaluation üîπ\")\n",
        "    print(f\"üìå Unique Words Before: {len(unique_original)}\")\n",
        "    print(f\"üìå Unique Words After Stemming: {len(unique_stemmed)}\")\n",
        "\n",
        "    # üîç Count How Many Words Changed\n",
        "    changed_words = [o for o, s in zip(original_tokens, stemmed_tokens) if o != s]\n",
        "    print(f\"üîç Words That Changed: {len(set(changed_words))} / {len(unique_original)} ({(len(set(changed_words)) / len(unique_original)) * 100:.2f}%)\")\n",
        "\n",
        "    # ‚ö†Ô∏è Check for Inconsistencies (same word, different stems)\n",
        "    stem_counts = Counter(stemmed_tokens)\n",
        "    inconsistent_words = {word for word, count in stem_counts.items() if count > 1}\n",
        "    print(f\"‚ö†Ô∏è Inconsistently Stemmed Words: {len(inconsistent_words)}\\n\")\n",
        "\n",
        "    # üîπ Reduction Rate\n",
        "    reduction_rate = ((len(unique_original) - len(unique_stemmed)) / len(unique_original)) * 100\n",
        "\n",
        "    # üîπ Consistency Score\n",
        "    consistency_score = sum(1 for token in original_tokens if stemmed_tokens.count(token) > 1) / len(original_tokens) * 100\n",
        "\n",
        "    return reduction_rate, consistency_score\n",
        "\n",
        "# ‚úÖ Apply Evaluation\n",
        "if tashaphyne_available:\n",
        "    df[\"tashaphyne_reduction_rate\"], df[\"tashaphyne_consistency_score\"] = zip(*df.apply(\n",
        "        lambda row: evaluate_stemming(row[\"filtered_tokens\"], row[\"tashaphyne_stemmed\"], \"Tashaphyne\"), axis=1\n",
        "    ))\n",
        "\n",
        "if khoja_available:\n",
        "    df[\"khoja_reduction_rate\"], df[\"khoja_consistency_score\"] = zip(*df.apply(\n",
        "        lambda row: evaluate_stemming(row[\"filtered_tokens\"], row[\"khoja_stemmed\"], \"Khoja\"), axis=1\n",
        "    ))\n",
        "\n",
        "# üîç Display Average Results\n",
        "if tashaphyne_available:\n",
        "    print(\"üìä Tashaphyne - Avg Reduction Rate:\", df[\"tashaphyne_reduction_rate\"].mean(), \"%\")\n",
        "    print(\"üìä Tashaphyne - Avg Consistency Score:\", df[\"tashaphyne_consistency_score\"].mean(), \"%\")\n",
        "\n",
        "if khoja_available:\n",
        "    print(\"üìä Khoja - Avg Reduction Rate:\", df[\"khoja_reduction_rate\"].mean(), \"%\")\n",
        "    print(\"üìä Khoja - Avg Consistency Score:\", df[\"khoja_consistency_score\"].mean(), \"%\")\n",
        "\n",
        "# ‚úÖ MADA (Requires Java & External Setup) - Not Implemented in This Script\n",
        "print(\"‚ö†Ô∏è MADA requires external Java setup and is not included in this script.\")\n"
      ],
      "metadata": {
        "id": "OBq1bBuzLcf7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##more testing\n",
        "##4-Isri 5-snowball both NLTK"
      ],
      "metadata": {
        "id": "0FLqCWZ0Owuh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import random\n",
        "from collections import Counter\n",
        "from nltk.stem.isri import ISRIStemmer\n",
        "from nltk.stem.snowball import ArabicStemmer\n",
        "\n",
        "# ‚úÖ Initialize Stemmers\n",
        "try:\n",
        "    isri_stemmer = ISRIStemmer()\n",
        "    snowball_stemmer = ArabicStemmer()\n",
        "    stemmers_available = True\n",
        "except Exception as e:\n",
        "    print(f\"‚ö†Ô∏è Stemmer Initialization Failed: {e}\")\n",
        "    stemmers_available = False\n",
        "\n",
        "# ‚úÖ Function to Apply Stemming\n",
        "\n",
        "def apply_stemming(tokens, stemmer, stemmer_name):\n",
        "    try:\n",
        "        return [stemmer.stem(word) for word in tokens]\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è {stemmer_name} Stemming Error: {e}\")\n",
        "        return tokens  # Return original tokens if error occurs\n",
        "\n",
        "if stemmers_available:\n",
        "    df[\"isri_stemmed_tokens\"] = df[\"filtered_tokens\"].apply(lambda tokens: apply_stemming(tokens, isri_stemmer, \"ISRI\"))\n",
        "    df[\"snowball_stemmed_tokens\"] = df[\"filtered_tokens\"].apply(lambda tokens: apply_stemming(tokens, snowball_stemmer, \"Snowball\"))\n",
        "\n",
        "# ‚úÖ Function to Evaluate Stemming Effectiveness\n",
        "def evaluate_stemming(df, original_column, stemmed_column, stemmer_name):\n",
        "    original_tokens = df[original_column].explode().dropna()\n",
        "    stemmed_tokens = df[stemmed_column].explode().dropna()\n",
        "\n",
        "    unique_original = set(original_tokens)\n",
        "    unique_stemmed = set(stemmed_tokens)\n",
        "\n",
        "    print(f\"üîπ {stemmer_name} Stemmer Evaluation üîπ\")\n",
        "    print(f\"üìå Unique Words Before: {len(unique_original)}\")\n",
        "    print(f\"üìå Unique Words After Stemming: {len(unique_stemmed)}\")\n",
        "\n",
        "    # üîç Count How Many Words Changed\n",
        "    changed_words = [o for o, s in zip(original_tokens, stemmed_tokens) if o != s]\n",
        "    print(f\"üîç Words That Changed: {len(set(changed_words))} / {len(unique_original)} ({(len(set(changed_words)) / len(unique_original)) * 100:.2f}%)\")\n",
        "\n",
        "    # ‚ö†Ô∏è Check for Inconsistencies (same word, different stems)\n",
        "    stem_counts = Counter(stemmed_tokens)\n",
        "    inconsistent_words = {word for word, count in stem_counts.items() if count > 1}\n",
        "    print(f\"‚ö†Ô∏è Inconsistently Stemmed Words: {len(inconsistent_words)}\\n\")\n",
        "\n",
        "    return changed_words, inconsistent_words\n",
        "\n",
        "# ‚úÖ Run Evaluation for Both Stemmers\n",
        "if stemmers_available:\n",
        "    changed_isri, inconsistent_isri = evaluate_stemming(df, \"filtered_tokens\", \"isri_stemmed_tokens\", \"ISRI\")\n",
        "    changed_snowball, inconsistent_snowball = evaluate_stemming(df, \"filtered_tokens\", \"snowball_stemmed_tokens\", \"Snowball\")\n",
        "\n",
        "    # ‚úÖ Show Random 5 Samples Before & After Stemming\n",
        "    samples = random.sample(range(len(df)), min(5, len(df)))  # Avoids sampling error if df has < 5 rows\n",
        "    print(\"üîç Sample Stemming Comparison üîç\\n\")\n",
        "    for i in samples:\n",
        "        print(f\"üîµ Original: {df['filtered_tokens'].iloc[i]}\")\n",
        "        print(f\"üü¢ ISRI Stemmed: {df['isri_stemmed_tokens'].iloc[i]}\")\n",
        "        print(f\"üü£ Snowball Stemmed: {df['snowball_stemmed_tokens'].iloc[i]}\")\n",
        "        print(\"-\" * 100)\n"
      ],
      "metadata": {
        "id": "3aK0peOvghCf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Trying 5-farasa again :( and 6- Camel tools again :("
      ],
      "metadata": {
        "id": "O5Kh84McO1vg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import random\n",
        "from collections import Counter\n",
        "\n",
        "# ‚úÖ Install required libraries\n",
        "!pip install farasapy transformers torch\n",
        "\n",
        "# ‚úÖ Import Lemmatization & Stemming Libraries\n",
        "from farasapy import FarasaStemmer\n",
        "from camel_tools.lemmatization import Lemmatizer\n",
        "from transformers import AutoModelForTokenClassification, AutoTokenizer, pipeline\n",
        "\n",
        "# ‚úÖ Initialize FarasaPy Stemmer (since lemmatization is unavailable) https://github.com/MagedSaeed/farasapy\n",
        "try:\n",
        "    farasa_stemmer = FarasaStemmer()\n",
        "    farasa_available = True\n",
        "except Exception as e:\n",
        "    print(f\"‚ö†Ô∏è FarasaPy (Stemming) initialization failed: {e}\")\n",
        "    farasa_available = False\n",
        "\n",
        "# ‚úÖ Initialize CAMeL Lemmatizer\n",
        "try:\n",
        "    camel_lemmatizer = Lemmatizer(model=\"default\")\n",
        "    camel_available = True\n",
        "except Exception as e:\n",
        "    print(f\"‚ö†Ô∏è CAMeL initialization failed: {e}\")\n",
        "    camel_available = False\n",
        "\n",
        "# ‚úÖ Initialize AraBERT Lemmatization Pipeline\n",
        "try:\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\"aubmindlab/bert-base-arabertv02\")\n",
        "    model = AutoModelForTokenClassification.from_pretrained(\"aubmindlab/bert-base-arabertv02\")\n",
        "    nlp_pipeline = pipeline(\"ner\", model=model, tokenizer=tokenizer)\n",
        "    arabert_available = True\n",
        "except Exception as e:\n",
        "    print(f\"‚ö†Ô∏è AraBERT initialization failed: {e}\")\n",
        "    arabert_available = False\n",
        "\n",
        "# ‚úÖ Function to Apply AraBERT Lemmatization\n",
        "def arabert_lemmatize(tokens):\n",
        "    lemmas = []\n",
        "    for token in tokens:\n",
        "        try:\n",
        "            result = nlp_pipeline(token)\n",
        "            lemma = result[0][\"word\"] if result else token  # Fallback to original token\n",
        "            lemmas.append(lemma)\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è AraBERT error on token '{token}': {e}\")\n",
        "            lemmas.append(token)  # Keep original token if error occurs\n",
        "    return lemmas\n",
        "\n",
        "# ‚úÖ Apply Lemmatization/Stemming with Error Handling\n",
        "if farasa_available:\n",
        "    try:\n",
        "        df[\"farasa_stemmed\"] = df[\"filtered_tokens\"].apply(lambda tokens: [farasa_stemmer.stem(word) for word in tokens])\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è FarasaPy Stemming failed: {e}\")\n",
        "        df[\"farasa_stemmed\"] = df[\"filtered_tokens\"]\n",
        "\n",
        "if camel_available:\n",
        "    try:\n",
        "        df[\"camel_lemmatized\"] = df[\"filtered_tokens\"].apply(lambda tokens: [camel_lemmatizer.lemmatize(word)[0] for word in tokens])\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è CAMeL Lemmatization failed: {e}\")\n",
        "        df[\"camel_lemmatized\"] = df[\"filtered_tokens\"]\n",
        "\n",
        "if arabert_available:\n",
        "    try:\n",
        "        df[\"arabert_lemmatized\"] = df[\"filtered_tokens\"].apply(lambda tokens: arabert_lemmatize(tokens))\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è AraBERT Lemmatization failed: {e}\")\n",
        "        df[\"arabert_lemmatized\"] = df[\"filtered_tokens\"]\n",
        "\n",
        "# ‚úÖ Function to Evaluate Lemmatization Effectiveness\n",
        "def evaluate_lemmatization(df, original_column, lemmatized_column, method_name):\n",
        "    original_tokens = df[original_column].explode().dropna()\n",
        "    lemmatized_tokens = df[lemmatized_column].explode().dropna()\n",
        "\n",
        "    # üìå Count Unique Words Before & After\n",
        "    unique_original = set(original_tokens)\n",
        "    unique_lemmatized = set(lemmatized_tokens)\n",
        "\n",
        "    print(f\"üîπ {method_name} Evaluation üîπ\")\n",
        "    print(f\"üìå Unique Words Before: {len(unique_original)}\")\n",
        "    print(f\"üìå Unique Words After Processing: {len(unique_lemmatized)}\")\n",
        "\n",
        "    # üîç Count How Many Words Changed\n",
        "    changed_words = [o for o, l in zip(original_tokens, lemmatized_tokens) if o != l]\n",
        "    print(f\"üîç Words That Changed: {len(set(changed_words))} / {len(unique_original)} ({(len(set(changed_words)) / len(unique_original)) * 100:.2f}%)\")\n",
        "\n",
        "    # ‚ö†Ô∏è Check for Inconsistencies (same word, different results)\n",
        "    lemma_counts = Counter(lemmatized_tokens)\n",
        "    inconsistent_words = {word for word, count in lemma_counts.items() if count > 1}\n",
        "    print(f\"‚ö†Ô∏è Inconsistently Processed Words: {len(inconsistent_words)}\\n\")\n",
        "\n",
        "    return changed_words, inconsistent_words\n",
        "\n",
        "# ‚úÖ Run Evaluation for Each Available Method\n",
        "if farasa_available:\n",
        "    changed_farasa, inconsistent_farasa = evaluate_lemmatization(df, \"filtered_tokens\", \"farasa_stemmed\", \"FarasaPy (Stemming)\")\n",
        "if camel_available:\n",
        "    changed_camel, inconsistent_camel = evaluate_lemmatization(df, \"filtered_tokens\", \"camel_lemmatized\", \"CAMeL (Lemmatization)\")\n",
        "if arabert_available:\n",
        "    changed_arabert, inconsistent_arabert = evaluate_lemmatization(df, \"filtered_tokens\", \"arabert_lemmatized\", \"AraBERT (Lemmatization)\")\n",
        "\n",
        "# ‚úÖ Show Random 5 Samples Before & After Processing\n",
        "samples = random.sample(range(len(df)), 5)\n",
        "print(\"üîç Sample Comparison üîç\\n\")\n",
        "for i in samples:\n",
        "    print(f\"üîµ Original: {df['filtered_tokens'].iloc[i]}\")\n",
        "    if farasa_available:\n",
        "        print(f\"üü¢ Farasa Stemmed: {df['farasa_stemmed'].iloc[i]}\")\n",
        "    if camel_available:\n",
        "        print(f\"üü£ CAMeL Lemmatized: {df['camel_lemmatized'].iloc[i]}\")\n",
        "    if arabert_available:\n",
        "        print(f\"üî¥ AraBERT Lemmatized: {df['arabert_lemmatized'].iloc[i]}\")\n",
        "    print(\"-\" * 100)\n"
      ],
      "metadata": {
        "id": "amRXmvkEJCre"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##After lemmatization we can now proceed to word embeddings so that ML models understand words. here we use **TF-IDF**"
      ],
      "metadata": {
        "id": "LMOsZh7fORu0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ‚úÖ Convert tokenized words into a string for each review\n",
        "df[\"filtered_text\"] = df[\"filtered_tokens\"].apply(lambda tokens: \" \".join(tokens))\n",
        "\n",
        "# ‚úÖ Apply TF-IDF Vectorization\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import pandas as pd\n",
        "\n",
        "# Initialize TF-IDF Vectorizer (Limit to top 15000 words)\n",
        "tfidf_vectorizer = TfidfVectorizer(max_features=15000)\n",
        "\n",
        "# Transform text into TF-IDF representation\n",
        "X_tfidf = tfidf_vectorizer.fit_transform(df[\"filtered_text\"])\n",
        "\n",
        "# Convert TF-IDF to DataFrame\n",
        "X_tfidf_df = pd.DataFrame(X_tfidf.toarray(), columns=tfidf_vectorizer.get_feature_names_out())\n",
        "\n",
        "# ‚úÖ Merge TF-IDF features with ratings\n",
        "df_tfidf_final = pd.concat([df[[\"rating\"]], X_tfidf_df], axis=1)\n",
        "\n",
        "# ‚úÖ Save the dataset (for future use)\n",
        "df_tfidf_final.to_csv(\"tfidf_with_ratings.csv\", index=False, encoding=\"utf-16\")\n",
        "print(\"‚úÖ TF-IDF dataset saved!\")\n",
        "\n",
        "# ‚úÖ Download the dataset in Colab\n",
        "from google.colab import files\n",
        "files.download(\"tfidf_with_ratings.csv\")\n",
        "print(\"üì• TF-IDF dataset downloaded successfully!\")\n",
        "\n",
        "# ‚úÖ Now use `df_tfidf_final` directly in this session without reloadin"
      ],
      "metadata": {
        "id": "J2Et3nbNs9bK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##We now have our reviews in a numerical form and ready for modelling"
      ],
      "metadata": {
        "id": "MQO7_va-PPm-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tpot\n",
        "\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from tpot import TPOTClassifier\n",
        "# import pickle\n",
        "# import numpy as np\n",
        "#both done already but left here for now\n",
        "\n",
        "#Use the in-session TF-IDF dataset instead of reloading from a file\n",
        "if \"df_tfidf_final\" not in globals():\n",
        "    raise ValueError(\"ERROR: df_tfidf_final not found. Ensure it is created before running TPOT.\")\n",
        "\n",
        "# Check dataset structure and handle missing values\n",
        "df_tfidf_final.dropna(inplace=True)  # Remove missing values if any\n",
        "print(df_tfidf_final.info())\n",
        "\n",
        "# Ensure correct data types\n",
        "X = df_tfidf_final.drop(columns=[\"rating\"]).astype(float)  # we drop rating from the reviews here\n",
        "y = df_tfidf_final[\"rating\"].astype(int)  # Ensure ratings are integers\n",
        "\n",
        "# conv ratings into a binary classification (Positive = 1, Negative = 0) as mentioned earlier (1,2 negative and 4,5 are positive with no neutral)\n",
        "y = y.apply(lambda x: 1 if x >= 4 else 0)\n",
        "\n",
        "#correct shapes\n",
        "print(f\"Feature Matrix Shape: {X.shape}, Target Vector Shape: {y.shape}\")\n",
        "\n",
        "# Split data into training & testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y # the norm\n",
        ")\n",
        "print(f\"Training Size: {X_train.shape}, Testing Size: {X_test.shape}\")\n",
        "\n",
        "tpot = TPOTClassifier() #init of the tpot classifier with default settings which are the following:\n",
        "# generations\t100\t            Number of iterations TPOT runs for optimization.\n",
        "# population_size\t100\t        Number of pipelines in each generation.\n",
        "# offspring_size\tNone        (same as population_size)\tNumber of new pipelines created per generation.\n",
        "# mutation_rate\t0.9\t          Probability of mutation in the genetic algorithm.\n",
        "# crossover_rate\t0.1\t        Probability of crossover (recombining pipelines).\n",
        "# scoring\t'accuracy'\t        Evaluation metric (can be changed to precision, recall, etc.).\n",
        "# cv\t5\t                      Default cross-validation strategy (5-fold CV).\n",
        "# subsample\t1.0\t              Fraction of data used during optimization.\n",
        "# n_jobs\t1\t                  Number of parallel jobs (-1 uses all CPU cores).\n",
        "# max_time_mins\tNone\t        No time limit by default.\n",
        "# max_eval_time_mins\t5\t      Maximum time per pipeline evaluation (in minutes).\n",
        "# early_stop\tNone\t          No early stopping by default.\n",
        "# config_dict\t'TPOT sparse'\t  Defines which models TPOT considers (e.g., classifiers, regressors).\n",
        "# warm_start\tFalse\t          If True, TPOT continues training from previous state.\n",
        "# memory\tNone\t              Caching mechanism for pipelines.\n",
        "# verbosity\t0\t                Controls logging level (0 = silent, 3 = detailed output).\n",
        "\n",
        "\n",
        "# Train TPOT to find the best model\n",
        "tpot.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate TPOT using cross-validation scores\n",
        "cv_scores = cross_val_score(tpot.fitted_pipeline_, X_train, y_train, cv=5)\n",
        "print(f\"‚úÖ Cross-validation Accuracy Scores: {cv_scores}\")\n",
        "print(f\"‚úÖ Mean CV Accuracy: {np.mean(cv_scores):.4f}\")\n",
        "\n",
        "# Evaluate TPOT on test data\n",
        "test_accuracy = tpot.score(X_test, y_test)\n",
        "print(f\"‚úÖ TPOT Best Model Test Accuracy: {test_accuracy:.4f}\")\n",
        "\n",
        "# Export the best model as a Python script\n",
        "tpot.export(\"best_tpot_pipeline.py\")\n",
        "\n",
        "# Save the trained best model for later use\n",
        "with open(\"tpot_best_model.pkl\", \"wb\") as f:\n",
        "    pickle.dump(tpot.fitted_pipeline_, f)\n",
        "print(\"‚úÖ TPOT best model saved successfully!\")\n"
      ],
      "metadata": {
        "id": "xF0y8cOQPYLr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FMMzNgwhCtsN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}